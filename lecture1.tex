\section{Lecture 1}
\label{lecture1}

\begin{center}
    \textbf{Content summary of the course}
\end{center}

Why study statistics specifically for stochastic processes? Because the first assumption in any statistics we have seen in the undergraduate course is that the data is iid (independent and identically distributed); this is not always the case (e.g., stock market data, epidemiology...). When data are ordered (usually through time) and are correlated between themselves, the standard statistical procedures cannot work correctly; we need a tool called \textbf{Time Series Analisys}. However, what is a time series?

Data are observed at discrete time from a stochastic dynamical system, usually described with a stochastic process.
\begin{definition}
    An \textbf{observed times series} are observation of a sequence of random variables indexed by a set of real numbers:
    \[ 
        (x_t)_{t\in{T}}, x_t \in \R, T \subset \R
    \]
    where $T=\{t_1, ..., t_n, ..\}$ is assumed to be discrete. If there is only one variable the then we have a \textbf{univariate} time series ($k=1$), otherwise we have a \textbf{multivariate} time series ($k>1$).
\end{definition}
\noindent{In general we also assume that $t\in\Z$, making the sampling point \textbf{equidistant}. The goal of a time series analisys is then to develop mathematical models that provide reasonable description of simple data.}

\begin{figure}[htbp]
    \centering
    \includegraphics[trim=30 20 20 20, clip, width=0.5\textwidth]{images/lecture1/time_serie.png}
    \caption{example of a simple time series}
\end{figure}

There are two main approaches to time series analysis, and they are not to be seen as exclusive of one another:
\begin{itemize}
    \item \textbf{Time domain approach}: assumes that correlation among adjacent points is best explained as the dependence of current value on past values; the present is seen as a parametric function of the past. The simplest of these models is the linear one: the current value of a variable is calculated as a linear regression of its past values. ARIMA (Autoregressive integrated moving average models) are an example of this approach that we will see later in the course. A more modern approach is the additive one, in which the observed variables are seen as a sum of different series.
    \item \textbf{Frequency domain approach}: this approach focuses on periodicity and sinusoidal variation of the interested data.
\end{itemize}
Typically the two approaches lead to very similar results, especially with much data available. Nevertheless, what distinguishes statistical analysis of iid data from time series analysis?

Recall the notion of joint pdf (probability density function):
Knowledge of $(X_t)_{t\in\Z} \Leftrightarrow $ joint distribution function $\prob{X_{t_1} \le x_1, ..., X_{t_n} \le x_n}, (x_1, ..., x_n) \in \R^n$ of $X_{t_1}, ..., X_{t_n}$ for any choiche of $t_1, ..., t_n \in \Z$ and for all $n \in \N$.
How is possible to estimate $\mathbb{P}$? If $(x_t)_{t\in\Z}$ are iid then $\mathbb{P}$ is fully specified by the density function $F_{X_0}(x)$ that can be calculated as the empirical marginal distribution:
\[
    F_n(x)=\frac{1}{n}\sum_{t=1}^{n}\mathds{1}_{\{x_t \le x\} \approx F_{X_0}(x)}
\]
with $\{x_1, ..., x_n\}$ sampled (Glivenko-Cantelli theorem). If $X_t$ are not iid there is no practical way to estimate $\mathbb{P}$ and the sample size has no more meaning; this is due to the \textbf{lack of stability}: distribution of $X_{t+1}, X_{t+2}, ..., X_{t_n}$ changes too much as a function of t so that the observed time series $x_1, ..., x_n$ is not sufficiently representative of $\mathbb{P}$ on $\R^\Z$. This implies that also the classical estimator for the mean $\expect{X_t}$ and the variance $Var(X_t)$ may not be significant.
\begin{example}
    Consider the sequence $(X_t)_{t\in\Z}$ such that $X_t=\mu t+W_t$ where $\mu > 0$, $(W_t)_{t\in\Z}$ are iid and distributed as $\mathcal{N}(0, \sigma^2)$. If we compute the sample mean we obtain:
    \begin{equation*}
    \begin{split}
        \mean{X}_n &= \frac{1}{n}\sum_{t=1}^{n}X_t\\
        &= \frac{1}{n}\sum_{t=1}^{n}(\mu t+W_t)\\
        &= \frac{\mu}{n}\sum_{t=1}^{n}t + \frac{1}{n}\sum_{t=1}^{n}W_t\\ 
        &= \frac{\mu}{n}\frac{n(n+1)}{2} + \mean{W}_t\\
        &= \mu\frac{n+1}{n} + \mean{W}_t\\
    \end{split}
    \end{equation*}
    Now, for the Law of Large Numbers:
    \[
        \lim_{n\to+\infty}(\mu\frac{n+1}{n} + \mean{W}_t) \xrightarrow{a.s.} +\infty
    \]
    This implies that it will be more useful to work with sequences of random variable with constant means; we can achieve this by considering a transformation like $X_t - \mu t = W_t$.
\end{example}
\begin{figure}[htbp]
    \centering
    \includegraphics[trim=0 0 0 0, clip, width=0.5\textwidth]{images/lecture1/transformation.png}
    \caption{example of the result of a transformation like $X_t - \mu t = W_t$}
\end{figure}

\begin{example}
    Consider the sequence $(X_t)_{t\in\Z}$ such that
    \begin{equation*}
        X_t=
        \begin{cases}
            0 & t \le 0\\
            \sum_{s=1}^{t}W_s &  t>0\\
        \end{cases} 
    \end{equation*}
    where $(W_t)_{t\in\Z}$ are iid and distributed as $\mathcal{N}(0, \sigma^2)$. The sample mean is then:
    \begin{equation*}
        \begin{split}
            \mean{X}_n &= \frac{1}{n}\sum_{t=1}^{n}X_t\\
            &= \frac{1}{n}\sum_{t=1}^{n}\sum_{s=1}^{t}W_s\\
            &= \frac{1}{n}[W_1 + (W_1+W_2) + ... + (W_1+...+W_n)]\\
            &= \frac{1}{n}[nW_1 + (n-1)W_2 + ... + W_n]\\
            &= \frac{1}{n}\sum_{t=1}^{n}(n-t+1)W_t
        \end{split}
    \end{equation*}
    Clearly $\expect{\mean{X}_t}=0$, since $\expect{W_t}=0$. Now let us compute the variance of the sample mean:
    \begin{equation*}
        \begin{split}
            Var({\mean{X}}_n) &= \frac{1}{n^2}Var(\sum_{j=1}^{n}jW_{n+1-j}) \leftarrow \text{we made an index charge here}\\
            &= \frac{1}{n^2}\sum_{j=1}^{n}j^2Var(W_{n+1-j})\\
            &= \frac{\sigma^2}{n^2}\sum_{j=1}^{n}j^2\\
            &= \frac{\sigma^2}{n^2} \frac{n(n+1)(2n+1)}{6}\\
            &= \frac{\sigma^2}{n} \frac{(n+1)(2n+1)}{6}
        \end{split}
    \end{equation*}
    But it can be seen that:
    \[
        \lim_{n\to+\infty}\frac{\sigma^2}{n} \frac{(n+1)(2n+1)}{6} \rightarrow +\infty
    \]
    This explains why classical statistical tools are not fit to time series: usually the sample mean is used because its variance tends to 0 as the sample size grows large, but this does not happen in the context of time series. It is clearly more useful to work with random variables that have a constant variance.
    \begin{remark}
        Applying the transformation $X_t-X_{t-1} = W_t$, with $t\in\Z$, it is possible to repeat the same calculation and obtain something more useful. We will study this later on.
    \end{remark}
\end{example}

Usually, the first step for a correct time series analysis is to produce a visual description of the data, like a plot with the observed values on the y-axis and the time on the x-axis. This could help to seek properties of the data like trends or seasonality. A common tool for doing this is the \textbf{scatter diagram}, which shows the correlations in the data. Another technique is the \textbf{decomposition}, which allows separating the noise from the signal, allowing, for example, looking for components explicable with different dynamics. These techniques allow us to separate a remainder (stochastic) component from the data. We will have to guess a model for it employing various techniques (Shapiro test, QQ plot, histogram...). If the data appears to be uncorrelated, it can be a case of \textbf{white noise}, that is a sequence of uncorrelated random variables. To assess the quality of the obtained model we can use \textbf{hypotesis testing} or the \textbf{AIC} and \textbf{BIC} criterion. The \textbf{forecasting} techniques exploit the stochastic model proposed to explain the data behavior up the given time to predict its dynamics after it. 
\begin{figure}[htbp]
    \centering
    \includegraphics[trim=0 0 0 0, clip, width=0.5\textwidth]{images/lecture1/scatter.png}
    \caption{example of a scatter plot}
\end{figure}

The \textbf{frequency domain approach} analyses the time series as a sum of sinusoidal components with uncorrelated random coefficients. This is called the \textbf{spectral representation} of the time series. The main tools for doing this are the \textbf{discrete Fourier transformations} and their statistical properties. An example of this approach could be
\[
    X_t=\sum_{k}a_k\sin(2\pi f_kt)+\sum_{k}b_k\cos(2\pi f_kt)    
\]
This approach clearly applyies naturally to data that have a periodic (cyclic) nature. A tool used in this context is the \textbf{periodogram} that graphs a measure of the relative importance of possible frequency values that might explain the oscillation pattern of the observed data.
\begin{figure}[htbp]
    \centering
    \includegraphics[trim=0 0 0 0, clip, width=0.5\textwidth]{images/lecture1/spectral.png}
    \caption{example of how a time series can be seen as a summation of trigonometrical functions}
\end{figure}
